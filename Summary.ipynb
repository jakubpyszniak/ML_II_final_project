{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36a151e",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 45px; text-align: center;\"><b>Machine Learning in Finance II - final project</b></p>\n",
    "<p style=\"font-size: 35px; text-align: center;\"><b>Forecasting delays in delivery time - Brazilian E-commerce</b></p>\n",
    "<p style=\"font-size: 20px; text-align: center;\"><b>Summary - choosing the best model</b></p>\n",
    "\n",
    "Author: Jakub Pyszniak\n",
    "\n",
    "Summary Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5afb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc547ea8",
   "metadata": {},
   "source": [
    "We utilise a similar custom RandomSearchCV set-up for cross-validation with some necessary modifications for each boosting model (especially for CatBoost due to slow performance) This ensures greater comparability of results however possibly ignores the full potential of indvidual tuning methods for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c7eb1",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494b04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy import stats\n",
    "import math\n",
    "import time\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Visual set-up\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# CatBoost\n",
    "from catboost import Pool, cv, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3bf05",
   "metadata": {},
   "source": [
    "> Quick ranking:\n",
    "\n",
    "1) CatBoost\n",
    "2) XGBoost\n",
    "3) LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cfd49",
   "metadata": {},
   "source": [
    "**Bring the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e424ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def eval_reg(model, X_test, y_test):\n",
    "    y_true = np.asarray(y_test).ravel()\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    r2   = float(r2_score(y_true, y_pred))\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175e2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = load_pickle(\"5.models/xgb25_top9_final_model.pkl\")\n",
    "lgbm_model = load_pickle(\"5.models/lgbm10vars_final_model.pkl\")\n",
    "cb_model = load_pickle(\"5.models/catboost30vars_final_cb.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfcf75",
   "metadata": {},
   "source": [
    "**And data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d390f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X10_test = load_pickle(\"6.data_cache/X10_test.pkl\")\n",
    "X10_train = load_pickle(\"6.data_cache/X10_train.pkl\")\n",
    "X25_test_top9 = load_pickle(\"6.data_cache/X25_test_top9.pkl\")\n",
    "X25_train_top9 = load_pickle(\"6.data_cache/X25_train_top9.pkl\")\n",
    "X30_test = load_pickle(\"6.data_cache/X30_test.pkl\")\n",
    "X30_train = load_pickle(\"6.data_cache/X30_train.pkl\")\n",
    "\n",
    "y_test = load_pickle(\"6.data_cache/y_test.pkl\")\n",
    "y_train = load_pickle(\"6.data_cache/y_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost_top9: RMSE=7.14608 | MAE=5.10731 | R2=0.52361\n",
      "LightGBM_10vars: RMSE=7.54629 | MAE=5.35523 | R2=0.46875\n",
      "CatBoost_30vars: RMSE=7.10868 | MAE=5.01791 | R2=0.52858\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# XGBoost model was trained on \"top9\" feature set\n",
    "results[\"XGBoost_top9\"] = eval_reg(xgb_model, X25_test_top9, y_test)\n",
    "\n",
    "# LightGBM model trained on 10-variable feature set\n",
    "results[\"LightGBM_10vars\"] = eval_reg(lgbm_model, X10_test, y_test)\n",
    "\n",
    "# CatBoost model trained on your 30-variable feature set \n",
    "results[\"CatBoost_30vars\"] = eval_reg(cb_model, X30_test, y_test) \n",
    "\n",
    "for name, (rmse, mae, r2) in results.items():\n",
    "    print(f\"{name}: RMSE={rmse:.5f} | MAE={mae:.5f} | R2={r2:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7bc3d",
   "metadata": {},
   "source": [
    "**We have the lowest RMSE for CatBoost!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a844f26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
